{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels = 6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ÎØ±',\n",
       " ';',\n",
       " '„ÖÅ',\n",
       " ';',\n",
       " 'ÏãúÍ∞ÑÏù¥',\n",
       " 'ÏóÜÏñ¥ÏÑú',\n",
       " 'ÏãúÎèÑ',\n",
       " '##ÎèÑ',\n",
       " 'ÏÇ¨Ïã§',\n",
       " 'Î™ªÌï¥',\n",
       " '##Î¥êÏÑú',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['ÎπÇ', 'Ïπª', 'Îø®' ,'·Ü¢', 'ÌÉÜ', 'ÎØ±', 'üëå' , '‚òº' ,'ÔΩÄ', 'Îµõ' ,'Íµ†', 'ÌÅ©', 'ÌõÉ' ,'Ï¶†'])\n",
    "tokenizer.tokenize('ÎØ± ;„ÖÅ; ÏãúÍ∞ÑÏù¥ ÏóÜÏñ¥ÏÑú ÏãúÎèÑÎèÑ ÏÇ¨Ïã§ Î™ªÌï¥Î¥êÏÑú...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50149"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.values())\n",
    "# tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraConfig {\n",
       "  \"_name_or_path\": \"beomi/KcELECTRA-base\",\n",
       "  \"architectures\": [\n",
       "    \"ElectraForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"embedding_size\": 768,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"electra\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"summary_activation\": \"gelu\",\n",
       "  \"summary_last_dropout\": 0.1,\n",
       "  \"summary_type\": \"first\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.21.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50135\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 226k/226k [00:00<00:00, 294kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:00<00:00, 56.9kB/s]\n",
      "Downloading tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 336/336 [00:00<00:00, 163kB/s]\n",
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.08k/1.08k [00:00<00:00, 493kB/s]\n",
      "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128M/128M [00:07<00:00, 19.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is ugly af\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anger'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is angryy\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(torch.tensor(5), num_classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "# input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5).softmax(dim=1)\n",
    "# output = loss(input, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([5])\n",
    "t = t.int()\n",
    "F.one_hot(torch.tensor(5), num_classes = 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [4],\n",
       "        [1],\n",
       "        [5],\n",
       "        [0]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([\n",
    "                [3],\n",
    "                [4],\n",
    "                [1],\n",
    "                [5],\n",
    "                [0],\n",
    "                  ])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(y, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4559)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.rand(2, 3, 4)\n",
    "target = torch.randint(4, (2, 3))\n",
    "\n",
    "# Reorder the dimensions\n",
    "# From: [time_step, batch_size, vocabulary_size]\n",
    "# To: [batch_size, vocabulary_size, time_step]\n",
    "predictions = predictions.permute(1, 2, 0)\n",
    "# From: [time_step, batch_size]\n",
    "# To: [batch_size, time_step]\n",
    "target = target.transpose(0, 1)\n",
    "\n",
    "F.cross_entropy(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6010, 0.2821, 0.3351, 0.9137, 0.1850, 0.3344, 0.0952, 0.1471, 0.0446,\n",
       "        0.9399, 0.7119, 0.9475, 0.9432, 0.9210, 0.3419, 0.9713, 0.8823, 0.8382,\n",
       "        0.1088, 0.5653, 0.8375, 0.5046, 0.2682, 0.5888, 0.3263, 0.1629, 0.3239,\n",
       "        0.4608, 0.2698, 0.1619, 0.4669, 0.9729])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(32,1)\n",
    "x.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0920, -0.0515, -0.2608,  1.1424,  1.1313],\n",
       "        [-1.6017,  0.5182,  1.3991, -2.1566, -0.8373],\n",
       "        [ 0.3238, -0.9305, -0.6168, -1.3069, -0.5969]], requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 2., 0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.argmax(dim=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "import dill\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        lr,\n",
    "        max_epoch,\n",
    "        warmup_ratio,\n",
    "        gradient_accumulation_steps,\n",
    "        tokenizer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "        self.max_epoch = max_epoch\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "\n",
    "        # ÏÇ¨Ïö©Ìï† Î™®Îç∏ÏùÑ Ìò∏Ï∂úÌï©ÎãàÎã§.\n",
    "        self.plm = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name, num_labels=6\n",
    "        )\n",
    "        self.plm.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.plm(x)[\"logits\"]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y.type(torch.LongTensor).squeeze().cuda())\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "    \n",
    "        logits = self(x)\n",
    "        loss = self.loss_func(logits, y.type(torch.LongTensor).squeeze().cuda())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\n",
    "            \"val_pearson\",\n",
    "            torchmetrics.functional.pearson_corrcoef(logits.argmax(dim=1).float().squeeze(), y.float().squeeze()),\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        self.log(\n",
    "            \"test_pearson\",\n",
    "            torchmetrics.functional.pearson_corrcoef(logits.argmax(dim=1).float().squeeze(), y.float().squeeze()),\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        logits = self(x)\n",
    "\n",
    "        return logits.squeeze()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        total_steps = (\n",
    "            len(self.trainer.datamodule.train_dataloader())\n",
    "            // self.gradient_accumulation_steps\n",
    "            * self.max_epoch\n",
    "        )\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=int(total_steps * self.warmup_ratio),\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "        optimizer_config = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        return optimizer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed locating file data.pkl: file not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mbase_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py:1045\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfind_class(mod_name, name)\n\u001b[1;32m   1044\u001b[0m \u001b[39m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m data_file \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(zip_file\u001b[39m.\u001b[39;49mget_record(pickle_file))\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed locating file data.pkl: file not found"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"base_model.pt\")\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
